# **Using Phi Family in Hugging Face**


[Hugging Face](https://huggingface.co/) is a very popular AI community with rich data and open source model resources. Different manufacturers will release open source LLM and SLM through Hugging Face, such as Microsoft, Meta, Mistral, Apple, Google, etc.

Microsoft Phi Family has been released on Hugging Face. Developers can download the corresponding Phi Family's model based on scenarios and businessesã€‚In addition to deploying Phi Pytorch models on Hugging Face, we also released quantized models, using GGUF and ONNX formats to give end users a choice.


## **Download Models in Hugging face**

You can download Phi family model whtih this link

-  **Phi-1 / 1.5** https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572

-  **Phi-3 / 3.5** https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3

-  **Phi-4** https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4

You can download the model in different ways, such as installing the ***Hugging face CLI SDK*** or use ***git clone***.

### **Using Hugginng face CLI to Download Phi Family model**

- Install Hugging face CLI

```bash

pip install -U "huggingface_hub[cli]"

```

- Using huggingface-cli to login

Login to Hugging face with [User Access Token](https://huggingface.co/docs/hub/security-tokens) from your [Settings page](https://huggingface.co/settings/tokens)


```bash

huggingface-cli login --token $HF_TOKEN --add-to-git-credential

```

- Download 


You can download model and save it to cache 

```bash

huggingface-cli download microsoft/phi-4

```

You can set location in your special location


```bash

huggingface-cli download microsoft/phi-4 --local-dir $YOUR_PATH

```


### **Using git clone to Download Phi Family model**

You can use ***git clone*** to download model too

```bash

git lfs install

git clone https://huggingface.co/microsoft/phi-4

```

## **Samples - Inference Microsoft Phi-4**

- **Installing transformers library**

```bash

pip install transformers -U

```

- **Running this code in VSCode**

```python

import transformers

pipeline = transformers.pipeline(
    "text-generation",
    model="microsoft/phi-4",
    model_kwargs={"torch_dtype": "auto"},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"},
]

outputs = pipeline(messages, max_new_tokens=2048)
print(outputs[0]["generated_text"][-1])

```





