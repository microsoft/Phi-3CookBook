# **在 Hugging Face 上使用 Phi 家族**

[Hugging Face](https://huggingface.co/) 是一个非常受欢迎的 AI 社区，拥有丰富的数据和开源模型资源。许多厂商会通过 Hugging Face 发布开源的 LLM 和 SLM，例如 Microsoft、Meta、Mistral、Apple、Google 等。

Microsoft 已经在 Hugging Face 上发布了 Phi 家族模型。开发者可以根据场景和业务需求下载相应的 Phi 家族模型。除了在 Hugging Face 上部署 Phi 的 Pytorch 模型外，我们还发布了量化模型，使用 GGUF 和 ONNX 格式，为终端用户提供了更多选择。

## **在 Hugging Face 上下载模型**

你可以通过以下链接下载 Phi 家族模型：

- **Phi-1 / 1.5** https://huggingface.co/collections/microsoft/phi-1-6626e29134744e94e222d572

- **Phi-3 / 3.5** https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3

- **Phi-4** https://huggingface.co/collections/microsoft/phi-4-677e9380e514feb5577a40e4

你可以通过多种方式下载模型，比如安装 ***Hugging Face CLI SDK*** 或使用 ***git clone***。

### **使用 Hugging Face CLI 下载 Phi 家族模型**

- 安装 Hugging Face CLI

```bash

pip install -U "huggingface_hub[cli]"

```

- 使用 huggingface-cli 登录

通过 [设置页面](https://huggingface.co/settings/tokens) 的 [用户访问令牌](https://huggingface.co/docs/hub/security-tokens) 登录 Hugging Face。

```bash

huggingface-cli login --token $HF_TOKEN --add-to-git-credential

```

- 下载

你可以下载模型并将其保存到缓存中：

```bash

huggingface-cli download microsoft/phi-4

```

你也可以将模型保存到指定的位置：

```bash

huggingface-cli download microsoft/phi-4 --local-dir $YOUR_PATH

```

### **使用 git clone 下载 Phi 家族模型**

你也可以使用 ***git clone*** 下载模型：

```bash

git lfs install

git clone https://huggingface.co/microsoft/phi-4

```

## **示例 - 推理 Microsoft Phi-4**

- **安装 transformers 库**

```bash

pip install transformers -U

```

- **在 VSCode 中运行以下代码**

```python

import transformers

pipeline = transformers.pipeline(
    "text-generation",
    model="microsoft/phi-4",
    model_kwargs={"torch_dtype": "auto"},
    device_map="auto",
)

messages = [
    {"role": "user", "content": "I have $20,000 in my savings account, where I receive a 4% profit per year and payments twice a year. Can you please tell me how long it will take for me to become a millionaire? Also, can you please explain the math step by step as if you were explaining it to an uneducated person?"},
]

outputs = pipeline(messages, max_new_tokens=2048)
print(outputs[0]["generated_text"][-1])

```

**免责声明**：  
本文件使用基于机器的人工智能翻译服务进行翻译。尽管我们尽力确保准确性，但请注意，自动翻译可能包含错误或不准确之处。应以原始语言的文件作为权威来源。对于关键信息，建议使用专业人工翻译。对于因使用此翻译而引起的任何误解或误读，我们概不负责。