# Belangrijke technologieën die genoemd worden zijn

1. [DirectML](https://learn.microsoft.com/windows/ai/directml/dml?WT.mc_id=aiml-138114-kinfeylo) - een low-level API voor hardwareversnelde machine learning, gebouwd bovenop DirectX 12.
2. [CUDA](https://blogs.nvidia.com/blog/what-is-cuda-2/) - een platform voor parallelle computing en een application programming interface (API)-model ontwikkeld door Nvidia, waarmee algemene verwerking op grafische verwerkingseenheden (GPU's) mogelijk is.
3. [ONNX](https://onnx.ai/) (Open Neural Network Exchange) - een open formaat ontworpen om machine learning-modellen te representeren en interoperabiliteit tussen verschillende ML-frameworks te bieden.
4. [GGUF](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md) (Generic Graph Update Format) - een formaat dat wordt gebruikt voor het representeren en bijwerken van machine learning-modellen, vooral handig voor kleinere taalmodellen die effectief kunnen draaien op CPU's met 4-8bit kwantisatie.

## DirectML

DirectML is een low-level API die hardwareversnelde machine learning mogelijk maakt. Het is gebouwd bovenop DirectX 12 om GPU-versnelling te benutten en is vendor-onafhankelijk, wat betekent dat er geen codewijzigingen nodig zijn om te werken met verschillende GPU-leveranciers. Het wordt voornamelijk gebruikt voor modeltraining en inferentie op GPU's.

Wat hardwareondersteuning betreft, is DirectML ontworpen om te werken met een breed scala aan GPU's, waaronder geïntegreerde en discrete AMD GPU's, geïntegreerde Intel GPU's en discrete NVIDIA GPU's. Het maakt deel uit van het Windows AI-platform en wordt ondersteund op Windows 10 & 11, waardoor modeltraining en inferentie op elk Windows-apparaat mogelijk zijn.

Er zijn updates en mogelijkheden geweest met betrekking tot DirectML, zoals ondersteuning voor tot wel 150 ONNX-operators en gebruik door zowel de ONNX-runtime als WinML. Het wordt ondersteund door grote Integrated Hardware Vendors (IHV's), die elk verschillende metacommands implementeren.

## CUDA

CUDA, wat staat voor Compute Unified Device Architecture, is een platform voor parallelle computing en een application programming interface (API)-model ontwikkeld door Nvidia. Het stelt softwareontwikkelaars in staat om een CUDA-compatibele grafische verwerkingseenheid (GPU) te gebruiken voor algemene verwerking – een aanpak die GPGPU (General-Purpose computing on Graphics Processing Units) wordt genoemd. CUDA is een belangrijke factor in Nvidia's GPU-versnelling en wordt veel gebruikt in diverse domeinen, zoals machine learning, wetenschappelijke berekeningen en videobewerking.

De hardwareondersteuning voor CUDA is specifiek voor Nvidia's GPU's, aangezien het een eigendomstechnologie is die door Nvidia is ontwikkeld. Elke architectuur ondersteunt specifieke versies van de CUDA-toolkit, die de benodigde bibliotheken en tools biedt voor ontwikkelaars om CUDA-toepassingen te bouwen en uit te voeren.

## ONNX

ONNX (Open Neural Network Exchange) is een open formaat ontworpen om machine learning-modellen te representeren. Het biedt een definitie van een uitbreidbaar computationeel grafiekmodel, evenals definities van ingebouwde operators en standaard datatypes. ONNX stelt ontwikkelaars in staat om modellen tussen verschillende ML-frameworks te verplaatsen, wat interoperabiliteit mogelijk maakt en het eenvoudiger maakt om AI-toepassingen te creëren en in te zetten.

Phi3 mini kan draaien met ONNX Runtime op CPU en GPU op verschillende apparaten, waaronder serverplatforms, Windows-, Linux- en Mac-desktops, en mobiele CPU's. De geoptimaliseerde configuraties die we hebben toegevoegd zijn:

- ONNX-modellen voor int4 DML: Gekwantiseerd naar int4 via AWQ
- ONNX-model voor fp16 CUDA
- ONNX-model voor int4 CUDA: Gekwantiseerd naar int4 via RTN
- ONNX-model voor int4 CPU en Mobiel: Gekwantiseerd naar int4 via RTN

## Llama.cpp

Llama.cpp is een open-source softwarebibliotheek geschreven in C++. Het voert inferentie uit op verschillende grote taalmodellen (LLM's), waaronder Llama. Ontwikkeld samen met de ggml-bibliotheek (een algemene tensorbibliotheek), streeft llama.cpp naar snellere inferentie en lager geheugengebruik in vergelijking met de oorspronkelijke Python-implementatie. Het ondersteunt hardwareoptimalisatie, kwantisatie en biedt een eenvoudige API en voorbeelden. Als je geïnteresseerd bent in efficiënte LLM-inferentie, is llama.cpp de moeite waard om te verkennen, aangezien Phi3 Llama.cpp kan draaien.

## GGUF

GGUF (Generic Graph Update Format) is een formaat dat wordt gebruikt voor het representeren en bijwerken van machine learning-modellen. Het is vooral nuttig voor kleinere taalmodellen (SLM's) die effectief kunnen draaien op CPU's met 4-8bit kwantisatie. GGUF is voordelig voor snelle prototyping en het draaien van modellen op edge-apparaten of in batchtaken zoals CI/CD-pijplijnen.

**Disclaimer**:  
Dit document is vertaald met behulp van AI-vertalingsdiensten. Hoewel we ons best doen om nauwkeurigheid te waarborgen, dient u zich ervan bewust te zijn dat geautomatiseerde vertalingen fouten of onnauwkeurigheden kunnen bevatten. Het originele document in de oorspronkelijke taal moet worden beschouwd als de gezaghebbende bron. Voor kritieke informatie wordt professionele menselijke vertaling aanbevolen. Wij zijn niet aansprakelijk voor misverstanden of verkeerde interpretaties die voortvloeien uit het gebruik van deze vertaling.