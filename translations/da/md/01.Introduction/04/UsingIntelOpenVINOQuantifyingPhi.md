# **Quantisering af Phi-3.5 med Intel OpenVINO**

Intel er den mest traditionelle CPU-producent med mange brugere. Med fremkomsten af maskinl칝ring og dyb l칝ring er Intel ogs친 tr친dt ind i konkurrencen om AI-acceleration. Til modelinference bruger Intel ikke kun GPU'er og CPU'er, men ogs친 NPU'er.

Vi h친ber at kunne implementere Phi-3.x-familien p친 slutbrugerenheder og g칮re den til en central del af AI-PC'er og Copilot-PC'er. Indl칝sningen af modellen p친 slutbrugerenheder afh칝nger af samarbejdet med forskellige hardwareproducenter. Dette kapitel fokuserer prim칝rt p친 anvendelsesscenariet for Intel OpenVINO som en kvantitativ model.

## **Hvad er OpenVINO**

OpenVINO er et open-source v칝rkt칮jss칝t til optimering og implementering af dyb l칝ringsmodeller fra skyen til kanten. Det accelererer dyb l칝ringsinference p친 tv칝rs af forskellige anvendelsestilf칝lde, s친som generativ AI, video, lyd og sprog, med modeller fra popul칝re rammer som PyTorch, TensorFlow, ONNX og flere. Konverter og optimer modeller, og implementer dem p친 en blanding af Intel춽 hardware og milj칮er, b친de lokalt og p친 enheder, i browseren eller i skyen.

Med OpenVINO kan du nu hurtigt kvantisere GenAI-modellen p친 Intel-hardware og accelerere modelinference.

OpenVINO underst칮tter nu kvantiseringskonvertering af Phi-3.5-Vision og Phi-3.5-Instruct.

### **Ops칝tning af milj칮**

S칮rg for, at f칮lgende milj칮afh칝ngigheder er installeret, dette er requirement.txt

```txt

--extra-index-url https://download.pytorch.org/whl/cpu
optimum-intel>=1.18.2
nncf>=2.11.0
openvino>=2024.3.0
transformers>=4.40
openvino-genai>=2024.3.0.0

```

### **Kvantisering af Phi-3.5-Instruct med OpenVINO**

I Terminal, k칮r venligst dette script

```bash


export llm_model_id = "microsoft/Phi-3.5-mini-instruct"

export llm_model_path = "your save quantizing Phi-3.5-instruct location"

optimum-cli export openvino --model {llm_model_id} --task text-generation-with-past --weight-format int4 --group-size 128 --ratio 0.6  --sym  --trust-remote-code {llm_model_path}


```

### **Kvantisering af Phi-3.5-Vision med OpenVINO**

K칮r venligst dette script i Python eller Jupyter Lab

```python

import requests
from pathlib import Path
from ov_phi3_vision import convert_phi3_model
import nncf

if not Path("ov_phi3_vision.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/ov_phi3_vision.py")
    open("ov_phi3_vision.py", "w").write(r.text)


if not Path("gradio_helper.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/notebooks/phi-3-vision/gradio_helper.py")
    open("gradio_helper.py", "w").write(r.text)

if not Path("notebook_utils.py").exists():
    r = requests.get(url="https://raw.githubusercontent.com/openvinotoolkit/openvino_notebooks/latest/utils/notebook_utils.py")
    open("notebook_utils.py", "w").write(r.text)



model_id = "microsoft/Phi-3.5-vision-instruct"
out_dir = Path("../model/phi-3.5-vision-128k-instruct-ov")
compression_configuration = {
    "mode": nncf.CompressWeightsMode.INT4_SYM,
    "group_size": 64,
    "ratio": 0.6,
}
if not out_dir.exists():
    convert_phi3_model(model_id, out_dir, compression_configuration)

```

### **游뱄 Eksempler for Phi-3.5 med Intel OpenVINO**

| Labs    | Introduktion | G친 til |
| -------- | ------- |  ------- |
| 游 Lab-Introduktion til Phi-3.5 Instruct  | L칝r hvordan du bruger Phi-3.5 Instruct p친 din AI-PC    |  [G친 til](../../../../../code/09.UpdateSamples/Aug/intel-phi35-instruct-zh.ipynb)    |
| 游 Lab-Introduktion til Phi-3.5 Vision (billede) | L칝r hvordan du bruger Phi-3.5 Vision til at analysere billeder p친 din AI-PC      |  [G친 til](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-img.ipynb)    |
| 游 Lab-Introduktion til Phi-3.5 Vision (video)   | L칝r hvordan du bruger Phi-3.5 Vision til at analysere videoer p친 din AI-PC    |  [G친 til](../../../../../code/09.UpdateSamples/Aug/intel-phi35-vision-video.ipynb)    |

## **Ressourcer**

1. L칝r mere om Intel OpenVINO [https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html](https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/overview.html)

2. Intel OpenVINO GitHub Repo [https://github.com/openvinotoolkit/openvino.genai](https://github.com/openvinotoolkit/openvino.genai)

**Ansvarsfraskrivelse**:  
Dette dokument er blevet oversat ved hj칝lp af maskinbaserede AI-overs칝ttelsestjenester. Selvom vi bestr칝ber os p친 n칮jagtighed, skal du v칝re opm칝rksom p친, at automatiserede overs칝ttelser kan indeholde fejl eller un칮jagtigheder. Det originale dokument p친 dets oprindelige sprog b칮r betragtes som den autoritative kilde. For kritisk information anbefales professionel menneskelig overs칝ttelse. Vi p친tager os intet ansvar for misforst친elser eller fejltolkninger, der m친tte opst친 ved brugen af denne overs칝ttelse.